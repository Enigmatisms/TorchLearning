## GAN 生成对抗网络

---

[toc]

---

#### 编码器原理

##### 自动编码器

​		自动编码器，简单地说就是以下结构：

``` mermaid
graph LR
A(Original)-->B(Encoder)
B-->C(Code-Compressed)
C-->D(Decoder)
D-->E(Generated)
F(Perturb & Intervene)-->C
```

​		从原始输入，对其进行编码（编码过程可以使用感知机（前馈，无BP操作）或者神经网络（现在的架构一般是BP式的优化）），生成 **<u>压缩后的编码数据</u>**。再想办法从编码中（必然存在信息损失）恢复原来的图片。但是一般而言，自动编码器（AE）都是确定输入输出的，每一种编码会由训练得到唯一的输出。

​		我们希望生成更多的数据或者达到人工智能创作 / 想象的目的，需要随机的输出。我们希望可以加入编码的扰动（Perturbance）或者人工的干预（Intervention），让编码更加多样化 / 随机化，而且Decoder可以对这种加入的噪声鲁棒。这就得到了VAE（Variational Auto Encoder）变分自动编码器。

##### 变分自动编码器

​		一个想法：首先一个随机的编码器在数学上的表现应该是给定一个向量（比如编码）Z，从Z中恢复X（训练集的分布）。那么就是从$P_Z(x)$到$P_X(x)$的映射（编码分布空间到样本空间的映射），得到新的分布$P_{\hat X}(x)$。那么根据贝叶斯的想法，可以表示分布$P_X(x)$为：
$$
P_X(x)=\sum_kP(X|Z_k)P(Z_k)
$$
​		意思是：不同编码的分布 与 给定编码下，输出数据为对应类型X 两者的条件概率结合。那么数据生成和分布之间的关系又是什么呢?直观地理解一下：

​		世界上有几乎无数种猫，猫产生的后代也不是和其父母一致的。那么我们如果想从数据层面【生成猫】，应该首先知道 **不同表现型的猫的【总体分布】**，如果知道这个分布，显然只需要从这个分布中随机采样就可以得到任意新性状的猫。但是这个分布基本是不可知的，即使存在海量数据我们也没办法得到这个分布的表达式。所以我们希望通过别的方式来近似表达这个分布，比如使用式(1)。

​		假设Z不是编码而是不同的形状，那么可以建立一个性状以及含有对应组合表现型的猫的概率映射，通过贝叶斯全概率公式获得。而把Z换为编码（**更加抽象的形状表示**），也是一样的，编码存在分布（正如不同性状如黑白存在一定分布），而给定编码（给定形状）时也存在其他的分布（Z=公猫，公猫中的白猫 / 花猫分布等等）。

​		VAE中，$P(Z)$被建模成了标准正态分布（其他分布也可），原因有以下两点：

- 标准正态分布常见，并且方便进行熵计算（**<u>或者说，KL散度计算时不会出现问题（比如均匀分布会存在概率密度为0导致奇异性的现象）</u>**）
- 天然的exp性质，并且表示容易，只需要对$\mu,\sigma$进行建模表示即可。

#### GAN原理

- GAN需要构建一个生成器和一个判别器，生成器需要生成能够以假乱真的数据。生成出的数据需要输入到判别器中，由判别器进行判定：此数据是真的（非生成的）还是假的（生成的）。**<u>固定判别器的参数，训练生成器参数，</u>**使得生成器生成的结果输入到判别器中，二分类（真假判定）得到结果尽可能接近1.
- 判别器的训练：我们希望我们的判别网络尽可能强大，能够区分真假数据，这样再进行生成器训练时，生成器训练会有更加严格的监督。判别器训练时，固定生成器网络参数，生成器生成的数据需要使label尽可能为false。

``` mermaid
graph LR
A(生成器训练)
B(判别器训练)
C(判别结果为1)
D(判别结果为0)
A-->C
C-->B
B-->D
D-->A

```

- 并且，分类器不仅能正确处理生成器生成的数据（正确地label成false类）（True Positive），还需要有处理True Negative的能力，对于真实的数据，需要正确分类为真。

---

#### GAN的数学原理

##### KL散度

​		需要回忆一下KL散度的定义，KL散度描述的是两个概率分布的相似程度，又称为相对熵。既然是“熵”，那么相对熵就与绝对熵（信息熵）存在关系。信息熵是信息不确定程度的度量。而信息量的度量是：
$$
I(x_i)=-log(p(x_i))
$$
​		也就是说：事件发生的概率越小，若发生，携带的信息量是越大的。

​		对于一个具有一定不确定度的信息源，一个事件$x_i$发生的概率若是$P(x_i)$，那么信息熵为：
$$
H(x_i)=-p(x_i)log(p(x_i))
$$
​		如果信息源发送的“事件”存在n个不同的取值，每个事件的概率为$p(x_i)$，那么，信息源熵为：
$$
H(U)=-\sum_{i=1}^np(x_i)log(p(x_i))
$$
​		可以看出，信息熵是系统信息量根据概率分布的加权，是一个系统的平均（期望）信息量。

KL散度的由来：与信道编码有关。

​		对于一个字符集（比如26字母，需要进行不等长编码），假设每个字符X出现的概率是$P(x)$，那么可以知道，一个字符需要编码的字节数（或者位数）会对等于信息量$I(x)$，那么一个字符集编码的平均字节数等于信息熵$H(U)$。假设，这个字符集的真实概率分布为$P(x_i)$，那么其平均编码数为
$$
H(X)=\sum_{i=1}^n p(x_i)log(\frac{1}{p(x_i)})
$$
​		由于，$P(x_i)$是字符集X的真实概率分布，对于X是最优的（对应的编码方式是最优的）。如果需要以这种编码方式对字符集Y（对应的概率分布为$Q(x_i)$）进行编码，编码平均字节数必然是更多的（因为不是最优的），那么这种编码下存在的差异是：
$$
D_{KL}(P||Q)=\sum P(x)log(\frac{1}{Q(x)}) - \sum P(x)log(\frac{1}{P(x)})=\sum P(x)\frac{P(x)}{Q(x)}
$$
​		理解了编码的物理意义就知道，为什么是$\frac {P(x)}{Q(x)}$了，因为$Q(x)$并非最优分布，需要的编码量更大。而我们不喜欢负数，为了让KL散度为正，故这样定义。

​		穿插一点优化原理的知识：为什么使用交叉熵作为很多网络的损失函数？

##### 交叉熵与KL散度的关系

​		交叉熵的定义：
$$
H_c(x)=-\sum_{i=1}^np(x_i)log(q(x_i))=\sum P(x)log(\frac{1}{Q(x)})
$$
​		与KL散度的公式进行对比，可以发现：
$$
D_{KL}(P||Q)=H_c(x)-\sum P(x)log(P(x))
$$
​		其中：$\sum P(x)log(P(x))$表征的是原始分布。在训练过程中（比如典型的分类器训练），$P(x)$一般是给定的：比如我们给定数据的原始label就是给定了一个原始分布，我们希望网络学习到的分布参数能够尽可能与原始分布接近。而由于$\sum P(x)log(P(x))$是常数（给定的原始分布是常的），优化交叉熵就相当于优化训练集与输出的KL散度。

##### 与GAN的关系

---

#### GAN分类器过强的原因



---

#### GAN实现过程中的一些问题

- 分类器过强导致gan_v2在无法判断终止的情况下，分类器一直进行训练，导致分类器的loss减到0.0001左右，而生成器的loss上涨到10左右。这样导致了生成器生成的图片完全为黑色图。
- GAN的数学原理，这个之后再推。
- WGAN的训练效果并不好，没有直接GAN的训练效果好。

