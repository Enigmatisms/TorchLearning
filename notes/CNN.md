## 交叉熵 / Softmax / Logit

---

#### 交叉熵

##### 信息熵

​		熵是信息量的衡量方式，一个随机事件的不确定度越大，说明信息量越大。由公式：
$$
H(x)=-\sum_{i=1}^np(x_i)log(p(x_i))
$$
​		对一个随机变量x的各种取值概率进行计算以获得信息熵。

##### 相对熵

​		KL散度就是相对熵（Kullback-Leibler divergence）。**很常用，用于定量描述一个随机变量的两个概率分布之间的距离。**

​		比如对于一个随机变量，它存在一个真实的概率分布p(x)，而我们假设了（或者学习了）一个概率分布q(x)。那么这两个概率分布之间的距离（接近程度）有多大呢？可以使用KL散度来描述：
$$
D_{KL}(p||q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)})
$$
​		上式描述了p对于q的相对熵。通常可以假设p为真实分布，q为预测分布。**最小化KL散度就是优化预测，使其更加接近真实分布**。

##### 交叉熵

​		（1）二分类情况：
$$
loss = \frac1{N}\sum_{i}[-y_ilog(p_i)-(1-y_i)log(1-p_i)]
$$
​		如何从物理意义上理解一下交叉熵？在二分类中，$y_i$为样本真实label，而$p_i$为样本被预测为正类的概率。可以知道，如果学习越准确，当样本为1是，$p_i$应该越接近1，那么交叉熵就越接近0，而反之如果真实label为1时，预测的概率很小，那么交叉熵就会非常大。对于负类而言也是如此。也就是说，当预测和真实值不符时，对于预测者来说信息量很大（与我们掌握的信息冲突了），这是不太好的。

​		多分类而言，需要拓展一下计算方式：对于M分类问题而言，每一个样本的计算：
$$
L=\sum_{i=1}^My_{c}log(p_c)
$$
​		其中，$y_c$为指标变量。当真实类别为c时$y_c$ = 1否则为0，而后面那部分就是我们预测的每一项的概率。对所有变量求和即可。

使用交叉熵与其他熵的等价性：

- 交叉熵的计算与KL散度存在关系（很简单的关系），可以发现分类问题中最小化交叉熵与最小化KL散度是一致的问题
- **<u>交叉熵等价于极大似然估计</u>**，使用最小化对数似然的方法，使模型趋近于样本分布。

---

#### Softmax

softmax的公式十分简单：

​		对于一个数组$V_N$(N元素)，其中第i个元素的元素值如果是$v_i$，那么softmax值就是：
$$
S(v_i)=\frac{e^{v_i}}{\sum_{j}^Ne^{v_j}}
$$
​		相当于一个按值归一的过程。显然生成的是一个概率！比如说我们的数组$V_N$是系统中粒子的能量，在平衡稳定的系统中，能量低的粒子出现概率大！那么可以通过能量+softmax映射得到不同能量的粒子出现的概率分布。（可能要用负指数，相当于分数（能量）取负值）

​		softmax和交叉熵是存在内在联系的！交叉熵如何获得概率的？根据softmax来的，式（4）中的$log(p_c)$就是：
$$
log(\frac{e^{v_i}}{\sum_{j}^Ne^{v_j}})
$$
​		那么也就是说，使用了softmax，又使用了CrossEntropyLoss相当于计算了两次softmax（至少在Pytorch里是这样的），注意这样很可能引起一些计算精度问题。

那么总结一下就是：

​		Softmax一般用于分类问题中，**<u>生成最后的类概率时使用此激活函数，可以将权重转化为概率分布。</u>**

##### Softmax引起的计算精度问题

​		在文件夹`problems`中的cnn_arch.pdf说明了softmax的问题，去掉softmax层后直接干到98.9准确率。为什么？由于softmax使用了 **<u>指数</u>**，当输出非常奇怪时（比如过大或者过小），那么指数操作将引起爆炸，造成上下溢出（over/under-flow），出现inf与-inf。这是完全不想要的。

​		要么使用归一化的方法避免softmax计算问题，要么就不要使用softmax + cross entropy loss。

---

#### logit

​		Log it。又叫做：**<u>Logistic Regression模型</u>**。

​		统计学概率：出现事件A次数 / 出现所有事件的总次数

​		**<u>比率（odds）</u>**：出现A次数 / 不出现A的次数（再取对数），写为概率形式就是：
$$
log(\frac{p_i}{1-p_i})
$$
​		这个式子也称为logit变换。

